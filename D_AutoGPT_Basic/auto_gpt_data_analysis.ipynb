{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "class TaskStatus(Enum):\n",
        "    PENDING = \"pending\"\n",
        "    IN_PROGRESS = \"in_progress\"\n",
        "    COMPLETED = \"completed\"\n",
        "    FAILED = \"failed\"\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    id: int\n",
        "    description: str\n",
        "    status: TaskStatus\n",
        "    code_to_execute: str = \"\"\n",
        "    result: str = \"\"\n",
        "    error: str = \"\"\n",
        "\n",
        "class PythonAutoGPT:\n",
        "    def __init__(self, objective: str):\n",
        "        self.objective = objective\n",
        "        self.tasks: List[Task] = []\n",
        "        self.task_counter = 0\n",
        "        self.max_iterations = 10\n",
        "        self.current_iteration = 0\n",
        "\n",
        "    def generate_tasks(self) -> List[str]:\n",
        "        \"\"\"Generate initial tasks based on the objective\"\"\"\n",
        "        # This is a simplified task generation - in a real AutoGPT,\n",
        "        # this would use an LLM to break down the objective.\n",
        "\n",
        "        common_patterns = {\n",
        "            \"data analysis\": [\n",
        "                \"Import necessary libraries (pandas, numpy, matplotlib)\",\n",
        "                \"Load or generate sample data\",\n",
        "                \"Perform basic data exploration\",\n",
        "                \"Create visualizations\",\n",
        "                \"Generate summary statistics\"\n",
        "            ],\n",
        "            \"web app\": [\n",
        "                \"Set up Flask/FastAPI application structure\",\n",
        "                \"Create basic routes and endpoints\",\n",
        "                \"Add HTML templates\",\n",
        "                \"Implement core functionality\",\n",
        "                \"Test the application\"\n",
        "            ],\n",
        "            \"algorithm\": [\n",
        "                \"Define the algorithm structure\",\n",
        "                \"Implement core logic\",\n",
        "                \"Create test cases\",\n",
        "                \"Optimize performance\",\n",
        "                \"Document the solution\"\n",
        "            ],\n",
        "            \"file processing\": [\n",
        "                \"Set up file handling utilities\",\n",
        "                \"Read and parse input files\",\n",
        "                \"Process data according to requirements\",\n",
        "                \"Generate output files\",\n",
        "                \"Validate results\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Simple keyword matching to determine task type.\n",
        "        objective_lower = self.objective.lower()\n",
        "\n",
        "        if any(word in objective_lower for word in [\"data\", \"analysis\", \"csv\", \"excel\"]):\n",
        "            return common_patterns[\"data analysis\"]\n",
        "        elif any(word in objective_lower for word in [\"web\", \"app\", \"server\", \"api\"]):\n",
        "            return common_patterns[\"web app\"]\n",
        "        elif any(word in objective_lower for word in [\"algorithm\", \"sort\", \"search\", \"optimize\"]):\n",
        "            return common_patterns[\"algorithm\"]\n",
        "        elif any(word in objective_lower for word in [\"file\", \"process\", \"convert\", \"parse\"]):\n",
        "            return common_patterns[\"file processing\"]\n",
        "        else:\n",
        "            # Default generic tasks.\n",
        "            return [\n",
        "                \"Analyze the objective and requirements\",\n",
        "                \"Design the solution approach\",\n",
        "                \"Implement core functionality\",\n",
        "                \"Test and validate the solution\",\n",
        "                \"Refine and optimize\"\n",
        "            ]\n",
        "\n",
        "    def create_task(self, description: str) -> Task:\n",
        "        \"\"\"Create a new task\"\"\"\n",
        "        self.task_counter += 1\n",
        "        task = Task(\n",
        "            id=self.task_counter,\n",
        "            description=description,\n",
        "            status=TaskStatus.PENDING\n",
        "        )\n",
        "        self.tasks.append(task)\n",
        "        return task\n",
        "\n",
        "    def generate_code_for_task(self, task: Task) -> str:\n",
        "        \"\"\"Generate Python code for a given task\"\"\"\n",
        "        # This is a simplified code generation - in a real AutoGPT,\n",
        "        # this would use an LLM to generate appropriate code.\n",
        "\n",
        "        task_desc = task.description.lower()\n",
        "\n",
        "        if \"import\" in task_desc and \"libraries\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure matplotlib for better plots\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 150\n",
        "plt.rcParams['savefig.bbox'] = 'tight'\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Matplotlib backend set to 'Agg' for file output\")\n",
        "\"\"\"\n",
        "        elif \"load\" in task_desc and \"data\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Generate sample data for demonstration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Check if sample_data folder exists and try to load real data\n",
        "if os.path.exists('sample_data'):\n",
        "    print(\"Found sample_data folder, attempting to load datasets...\")\n",
        "    data_files = []\n",
        "    for file in os.listdir('sample_data'):\n",
        "        if file.endswith('.csv'):\n",
        "            data_files.append(file)\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found CSV files: {data_files}\")\n",
        "        # Load the first CSV file found\n",
        "        file_path = os.path.join('sample_data', data_files[0])\n",
        "        print(f\"Loading: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Real data loaded with shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"No CSV files found in sample_data folder\")\n",
        "        # Fallback to generated data\n",
        "        np.random.seed(42)\n",
        "        data = {\n",
        "            'date': pd.date_range('2024-01-01', periods=100),\n",
        "            'value': np.random.randn(100).cumsum(),\n",
        "            'category': np.random.choice(['A', 'B', 'C'], 100)\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"Generated sample data with shape: {df.shape}\")\n",
        "        print(df.head())\n",
        "else:\n",
        "    print(\"sample_data folder not found, generating sample data...\")\n",
        "    # Create sample dataset\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'date': pd.date_range('2024-01-01', periods=100),\n",
        "        'sales': np.random.uniform(1000, 10000, 100),\n",
        "        'temperature': np.random.normal(25, 10, 100),\n",
        "        'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 100),\n",
        "        'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
        "        'price': np.random.uniform(10, 1000, 100),\n",
        "        'score': np.random.normal(75, 15, 100)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"Sample data created with shape: {df.shape}\")\n",
        "    print(df.head())\n",
        "\n",
        "# Ensure we have the dataframe for later use\n",
        "print(f\"\\\\nDataFrame 'df' is ready with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "\"\"\"\n",
        "        elif \"exploration\" in task_desc or \"explore\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Data exploration\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"DATA EXPLORATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\\\nLast few rows:\")\n",
        "print(df.tail())\n",
        "\n",
        "# Basic statistics for numerical columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    print(f\"\\\\nNumerical columns: {list(numeric_cols)}\")\n",
        "    print(\"\\\\nBasic statistics:\")\n",
        "    print(df[numeric_cols].describe())\n",
        "\n",
        "# Unique values for categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"\\\\nCategorical columns: {list(categorical_cols)}\")\n",
        "    for col in categorical_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"\\\\n{col}: {unique_count} unique values\")\n",
        "        if unique_count <= 10:  # Show values if not too many\n",
        "            print(df[col].value_counts())\n",
        "        else:\n",
        "            print(f\"Top 5 most frequent values:\")\n",
        "            print(df[col].value_counts().head())\n",
        "\"\"\"\n",
        "        elif \"visualization\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Create visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Ensure we're using the Agg backend\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Create output directory for plots\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Set style for better looking plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Get numeric and categorical columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Filter out datetime columns from numeric columns\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in datetime_cols]\n",
        "\n",
        "print(f\"Creating visualizations for {len(numeric_cols)} numeric columns and {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "# 1. DISTRIBUTION PLOTS FOR NUMERIC COLUMNS\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"\\\\n1. Creating distribution plots for numeric columns...\")\n",
        "\n",
        "    # Calculate subplot layout\n",
        "    n_numeric = min(4, len(numeric_cols))  # Limit to 4 plots\n",
        "    if n_numeric == 1:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(8, 6))\n",
        "        axes = [axes]\n",
        "    elif n_numeric == 2:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(numeric_cols[:n_numeric]):\n",
        "        try:\n",
        "            # Create histogram\n",
        "            data_clean = df[col].dropna()\n",
        "            axes[i].hist(data_clean, bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "            axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "            axes[i].set_xlabel(col)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "            # Add statistics text\n",
        "            mean_val = data_clean.mean()\n",
        "            median_val = data_clean.median()\n",
        "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
        "            axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
        "            axes[i].legend()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting {col}: {e}\")\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(n_numeric, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/numeric_distributions.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.close()\n",
        "    print(\"   ✅ Numeric distributions saved as 'plots/numeric_distributions.png'\")\n",
        "\n",
        "# 2. CATEGORICAL PLOTS\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\\\n2. Creating categorical distribution plots...\")\n",
        "\n",
        "    n_categorical = min(2, len(categorical_cols))\n",
        "    if n_categorical == 1:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    for i, col in enumerate(categorical_cols[:n_categorical]):\n",
        "        try:\n",
        "            value_counts = df[col].value_counts().head(10)  # Top 10 categories\n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(value_counts)))\n",
        "\n",
        "            bars = axes[i].bar(range(len(value_counts)), value_counts.values, color=colors)\n",
        "            axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "            axes[i].set_xlabel(col)\n",
        "            axes[i].set_ylabel('Count')\n",
        "            axes[i].set_xticks(range(len(value_counts)))\n",
        "            axes[i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
        "            axes[i].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, count in zip(bars, value_counts.values):\n",
        "                height = bar.get_height()\n",
        "                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                           f'{int(count)}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting {col}: {e}\")\n",
        "\n",
        "    # Hide unused subplot if only one categorical column\n",
        "    if n_categorical == 1 and len(axes) > 1:\n",
        "        axes[1].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/categorical_distributions.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.close()\n",
        "    print(\"   ✅ Categorical distributions saved as 'plots/categorical_distributions.png'\")\n",
        "\n",
        "# 3. CORRELATION HEATMAP\n",
        "if len(numeric_cols) > 1:\n",
        "    print(\"\\\\n3. Creating correlation heatmap...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Create heatmap\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Hide upper triangle\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
        "\n",
        "    plt.title('Correlation Matrix of Numeric Variables', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/correlation_heatmap.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.close()\n",
        "    print(\"   ✅ Correlation heatmap saved as 'plots/correlation_heatmap.png'\")\n",
        "\n",
        "# 4. SCATTER PLOTS\n",
        "if len(numeric_cols) >= 2:\n",
        "    print(\"\\\\n4. Creating scatter plots...\")\n",
        "\n",
        "    # Create scatter plot for first two numeric columns\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    x_col = numeric_cols[0]\n",
        "    y_col = numeric_cols[1]\n",
        "\n",
        "    # Create scatter plot with color coding if we have categorical data\n",
        "    if len(categorical_cols) > 0:\n",
        "        # Color by first categorical column\n",
        "        color_col = categorical_cols[0]\n",
        "        unique_categories = df[color_col].unique()[:10]  # Limit to 10 categories\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))\n",
        "\n",
        "        for i, category in enumerate(unique_categories):\n",
        "            mask = df[color_col] == category\n",
        "            plt.scatter(df[mask][x_col], df[mask][y_col],\n",
        "                       alpha=0.6, s=50, color=colors[i], label=str(category))\n",
        "\n",
        "        plt.legend(title=color_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    else:\n",
        "        plt.scatter(df[x_col], df[y_col], alpha=0.6, s=50, color='coral')\n",
        "\n",
        "    plt.xlabel(x_col, fontsize=12)\n",
        "    plt.ylabel(y_col, fontsize=12)\n",
        "    plt.title(f'Scatter Plot: {x_col} vs {y_col}', fontsize=14, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/scatter_plot.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.close()\n",
        "    print(\"   ✅ Scatter plot saved as 'plots/scatter_plot.png'\")\n",
        "\n",
        "# 5. BOX PLOTS (if we have both numeric and categorical data)\n",
        "if len(numeric_cols) > 0 and len(categorical_cols) > 0:\n",
        "    print(\"\\\\n5. Creating box plots...\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Use first numeric and first categorical column\n",
        "    numeric_col = numeric_cols[0]\n",
        "    categorical_col = categorical_cols[0]\n",
        "\n",
        "    # Create box plot\n",
        "    df.boxplot(column=numeric_col, by=categorical_col, ax=ax, figsize=(12, 6))\n",
        "    plt.suptitle('')  # Remove the automatic title\n",
        "    plt.title(f'Box Plot: {numeric_col} by {categorical_col}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(categorical_col, fontsize=12)\n",
        "    plt.ylabel(numeric_col, fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/box_plots.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    plt.close()\n",
        "    print(\"   ✅ Box plots saved as 'plots/box_plots.png'\")\n",
        "\n",
        "# 6. SUMMARY DASHBOARD\n",
        "print(\"\\\\n6. Creating summary dashboard...\")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Create a 3x2 grid\n",
        "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Dataset overview (text)\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.axis('off')\n",
        "overview_text = f'''Dataset Overview:\n",
        "• Shape: {df.shape[0]} rows × {df.shape[1]} columns\n",
        "• Numeric columns: {len(numeric_cols)}\n",
        "• Categorical columns: {len(categorical_cols)}\n",
        "• Missing values: {df.isnull().sum().sum()}\n",
        "• Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB'''\n",
        "\n",
        "ax1.text(0.05, 0.95, overview_text, transform=ax1.transAxes,\n",
        "         fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "# Plot 2: First numeric column histogram\n",
        "if len(numeric_cols) > 0:\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    col = numeric_cols[0]\n",
        "    ax2.hist(df[col].dropna(), bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax2.set_title(f'Distribution: {col}', fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: First categorical column\n",
        "if len(categorical_cols) > 0:\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    col = categorical_cols[0]\n",
        "    value_counts = df[col].value_counts().head(8)\n",
        "    ax3.bar(range(len(value_counts)), value_counts.values, color='lightcoral')\n",
        "    ax3.set_title(f'Top Categories: {col}', fontweight='bold')\n",
        "    ax3.set_xticks(range(len(value_counts)))\n",
        "    ax3.set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 4: Correlation heatmap (small version)\n",
        "if len(numeric_cols) > 1:\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "    im = ax4.imshow(corr_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
        "    ax4.set_title('Correlation Matrix', fontweight='bold')\n",
        "    ax4.set_xticks(range(len(corr_matrix.columns)))\n",
        "    ax4.set_yticks(range(len(corr_matrix.columns)))\n",
        "    ax4.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
        "    ax4.set_yticklabels(corr_matrix.columns)\n",
        "\n",
        "# Plot 5: Scatter plot\n",
        "if len(numeric_cols) >= 2:\n",
        "    ax5 = fig.add_subplot(gs[2, :])\n",
        "    x_col, y_col = numeric_cols[0], numeric_cols[1]\n",
        "    ax5.scatter(df[x_col], df[y_col], alpha=0.6, s=30, color='green')\n",
        "    ax5.set_xlabel(x_col)\n",
        "    ax5.set_ylabel(y_col)\n",
        "    ax5.set_title(f'Scatter: {x_col} vs {y_col}', fontweight='bold')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Data Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "plt.savefig('plots/dashboard.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.close()\n",
        "print(\"   ✅ Dashboard saved as 'plots/dashboard.png'\")\n",
        "\n",
        "# List all created files\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"📊 VISUALIZATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "plot_files = []\n",
        "plots_dir = 'plots'\n",
        "if os.path.exists(plots_dir):\n",
        "    for file in os.listdir(plots_dir):\n",
        "        if file.endswith('.png'):\n",
        "            file_path = os.path.join(plots_dir, file)\n",
        "            file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "            plot_files.append((file, file_size))\n",
        "\n",
        "if plot_files:\n",
        "    print(\"Generated visualization files:\")\n",
        "    for filename, size in plot_files:\n",
        "        print(f\"  ✅ {filename} ({size:.1f} KB)\")\n",
        "else:\n",
        "    print(\"  ❌ No plot files found\")\n",
        "\n",
        "print(f\"\\\\nTotal plots created: {len(plot_files)}\")\n",
        "print(\"All visualizations are saved in the 'plots/' directory\")\n",
        "\"\"\"\n",
        "        elif \"summary\" in task_desc and \"statistics\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Generate comprehensive summary statistics\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE SUMMARY STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset overview\n",
        "print(f\"Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Numerical summary\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    print(f\"\\\\n📊 NUMERICAL VARIABLES ({len(numeric_cols)} columns)\")\n",
        "    print(\"-\" * 50)\n",
        "    print(df[numeric_cols].describe())\n",
        "\n",
        "    # Additional statistics\n",
        "    print(\"\\\\nAdditional Statistics:\")\n",
        "    for col in numeric_cols:\n",
        "        print(f\"\\\\n{col}:\")\n",
        "        print(f\"  • Range: {df[col].min():.3f} to {df[col].max():.3f}\")\n",
        "        print(f\"  • IQR: {df[col].quantile(0.75) - df[col].quantile(0.25):.3f}\")\n",
        "        try:\n",
        "            print(f\"  • Skewness: {df[col].skew():.3f}\")\n",
        "        except:\n",
        "            print(f\"  • Skewness: Unable to calculate\")\n",
        "        print(f\"  • Missing values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Categorical summary\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"\\\\n📋 CATEGORICAL VARIABLES ({len(categorical_cols)} columns)\")\n",
        "    print(\"-\" * 50)\n",
        "    for col in categorical_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else \"N/A\"\n",
        "        print(f\"\\\\n{col}:\")\n",
        "        print(f\"  • Unique values: {unique_count}\")\n",
        "        print(f\"  • Most frequent: '{mode_value}' ({df[col].value_counts().iloc[0]} times)\")\n",
        "        print(f\"  • Missing values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "        if unique_count <= 10:\n",
        "            print(f\"  • Value distribution:\")\n",
        "            for value, count in df[col].value_counts().items():\n",
        "                percentage = count / len(df) * 100\n",
        "                print(f\"    - '{value}': {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# Correlation analysis for numerical variables\n",
        "if len(numeric_cols) > 1:\n",
        "    print(f\"\\\\n🔗 CORRELATION ANALYSIS\")\n",
        "    print(\"-\" * 50)\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Find highly correlated pairs\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_value = corr_matrix.iloc[i, j]\n",
        "            if abs(corr_value) > 0.5:  # Threshold for \"high\" correlation\n",
        "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_value))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(\"High correlations (|r| > 0.5):\")\n",
        "        for col1, col2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "            print(f\"  • {col1} ↔ {col2}: r = {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"No strong correlations found (all |r| ≤ 0.5)\")\n",
        "\n",
        "# Data quality assessment\n",
        "print(f\"\\\\n🔍 DATA QUALITY ASSESSMENT\")\n",
        "print(\"-\" * 50)\n",
        "total_missing = df.isnull().sum().sum()\n",
        "print(f\"Total missing values: {total_missing} ({total_missing/(df.shape[0]*df.shape[1])*100:.2f}% of all values)\")\n",
        "\n",
        "if total_missing > 0:\n",
        "    print(\"\\\\nColumns with missing values:\")\n",
        "    missing_cols = df.isnull().sum()\n",
        "    missing_cols = missing_cols[missing_cols > 0].sort_values(ascending=False)\n",
        "    for col, count in missing_cols.items():\n",
        "        print(f\"  • {col}: {count} missing ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"\\\\nDuplicate rows: {duplicate_count} ({duplicate_count/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Generate insights\n",
        "print(f\"\\\\n💡 KEY INSIGHTS\")\n",
        "print(\"-\" * 50)\n",
        "insights = []\n",
        "\n",
        "# Check for highly correlated variables\n",
        "if len(high_corr_pairs) > 0:\n",
        "    insights.append(f\"Found {len(high_corr_pairs)} highly correlated variable pairs\")\n",
        "\n",
        "# Check for missing data\n",
        "if total_missing > 0:\n",
        "    insights.append(f\"Dataset has {total_missing} missing values across {len(missing_cols)} columns\")\n",
        "\n",
        "# Check for duplicates\n",
        "if duplicate_count > 0:\n",
        "    insights.append(f\"Dataset contains {duplicate_count} duplicate rows\")\n",
        "\n",
        "# Check data distribution\n",
        "if len(numeric_cols) > 0:\n",
        "    highly_skewed = []\n",
        "    for col in numeric_cols:\n",
        "        try:\n",
        "            skewness = abs(df[col].skew())\n",
        "            if skewness > 2:\n",
        "                highly_skewed.append(col)\n",
        "        except:\n",
        "            pass\n",
        "    if highly_skewed:\n",
        "        insights.append(f\"Highly skewed variables detected: {', '.join(highly_skewed)}\")\n",
        "\n",
        "if insights:\n",
        "    for i, insight in enumerate(insights, 1):\n",
        "        print(f\"{i}. {insight}\")\n",
        "else:\n",
        "    print(\"• Dataset appears to be well-structured with no major quality issues\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPLETE! 🎉\")\n",
        "print(\"=\"*60)\n",
        "\"\"\"\n",
        "        else:\n",
        "            # Generic task code\n",
        "            return f\"\"\"\n",
        "# Task: {task.description}\n",
        "print(\"Executing task: {task.description}\")\n",
        "print(\"Task implementation would go here...\")\n",
        "print(\"Task completed successfully!\")\n",
        "\"\"\"\n",
        "\n",
        "    def execute_code(self, code: str) -> tuple[str, str]:\n",
        "        \"\"\"Execute Python code and return output and error\"\"\"\n",
        "        try:\n",
        "            # Build cumulative code by combining all previous successful task codes.\n",
        "            cumulative_code = \"\"\n",
        "\n",
        "            # Add all previously executed successful task codes.\n",
        "            for task in self.tasks:\n",
        "                if task.status == TaskStatus.COMPLETED and task.code_to_execute:\n",
        "                    cumulative_code += task.code_to_execute + \"\\n\\n\"\n",
        "\n",
        "            # Add current code.\n",
        "            full_code = cumulative_code + code\n",
        "\n",
        "            # Create a temporary file to execute the code.\n",
        "            with open('temp_code.py', 'w') as f:\n",
        "                f.write(full_code)\n",
        "\n",
        "            # Execute the code and capture output.\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, 'temp_code.py'],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            # Clean up\n",
        "            if os.path.exists('temp_code.py'):\n",
        "                os.remove('temp_code.py')\n",
        "\n",
        "            return result.stdout, result.stderr\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return \"\", \"Code execution timed out\"\n",
        "        except Exception as e:\n",
        "            return \"\", f\"Execution error: {str(e)}\"\n",
        "\n",
        "    def execute_task(self, task: Task) -> bool:\n",
        "        \"\"\"Execute a single task\"\"\"\n",
        "        print(f\"\\\\n--- Executing Task {task.id}: {task.description} ---\")\n",
        "\n",
        "        task.status = TaskStatus.IN_PROGRESS\n",
        "\n",
        "        # Generate code for the task.\n",
        "        task.code_to_execute = self.generate_code_for_task(task)\n",
        "\n",
        "        print(\"Generated code:\")\n",
        "        print(\"```python\")\n",
        "        print(task.code_to_execute)\n",
        "        print(\"```\")\n",
        "\n",
        "        # Execute the code.\n",
        "        output, error = self.execute_code(task.code_to_execute)\n",
        "\n",
        "        task.result = output\n",
        "        task.error = error\n",
        "\n",
        "        if error:\n",
        "            print(f\"❌ Task failed with error: {error}\")\n",
        "            task.status = TaskStatus.FAILED\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"✅ Task completed successfully!\")\n",
        "            if output:\n",
        "                print(\"Output:\")\n",
        "                print(output)\n",
        "            task.status = TaskStatus.COMPLETED\n",
        "            return True\n",
        "\n",
        "    def should_continue(self) -> bool:\n",
        "        \"\"\"Determine if the agent should continue with more tasks\"\"\"\n",
        "        if self.current_iteration >= self.max_iterations:\n",
        "            print(f\"\\\\n🛑 Reached maximum iterations ({self.max_iterations})\")\n",
        "            return False\n",
        "\n",
        "        pending_tasks = [t for t in self.tasks if t.status == TaskStatus.PENDING]\n",
        "        if not pending_tasks:\n",
        "            print(\"\\\\n✅ All tasks completed!\")\n",
        "            return False\n",
        "\n",
        "        failed_tasks = [t for t in self.tasks if t.status == TaskStatus.FAILED]\n",
        "        if len(failed_tasks) > len(self.tasks) * 0.5:  # If more than 50% failed\n",
        "            print(\"\\\\n❌ Too many tasks failed, stopping execution\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main execution loop\"\"\"\n",
        "        print(f\"🚀 Starting AutoGPT Agent\")\n",
        "        print(f\"Objective: {self.objective}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Generate initial tasks.\n",
        "        task_descriptions = self.generate_tasks()\n",
        "        for desc in task_descriptions:\n",
        "            self.create_task(desc)\n",
        "\n",
        "        print(f\"\\\\n📋 Generated {len(self.tasks)} tasks:\")\n",
        "        for task in self.tasks:\n",
        "            print(f\"  {task.id}. {task.description}\")\n",
        "\n",
        "        # Execute tasks.\n",
        "        while self.should_continue():\n",
        "            self.current_iteration += 1\n",
        "            print(f\"\\\\n🔄 Iteration {self.current_iteration}\")\n",
        "\n",
        "            # Find next pending task.\n",
        "            pending_tasks = [t for t in self.tasks if t.status == TaskStatus.PENDING]\n",
        "            if not pending_tasks:\n",
        "                break\n",
        "\n",
        "            current_task = pending_tasks[0]\n",
        "            success = self.execute_task(current_task)\n",
        "\n",
        "            # Brief pause between tasks.\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Final summary.\n",
        "        self.print_summary()\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print execution summary\"\"\"\n",
        "        print(\"\\\\n\" + \"=\" * 60)\n",
        "        print(\"📊 EXECUTION SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        completed = len([t for t in self.tasks if t.status == TaskStatus.COMPLETED])\n",
        "        failed = len([t for t in self.tasks if t.status == TaskStatus.FAILED])\n",
        "        pending = len([t for t in self.tasks if t.status == TaskStatus.PENDING])\n",
        "\n",
        "        print(f\"Objective: {self.objective}\")\n",
        "        print(f\"Total Tasks: {len(self.tasks)}\")\n",
        "        print(f\"✅ Completed: {completed}\")\n",
        "        print(f\"❌ Failed: {failed}\")\n",
        "        print(f\"⏳ Pending: {pending}\")\n",
        "        print(f\"🔄 Iterations: {self.current_iteration}\")\n",
        "\n",
        "        print(\"\\\\nTask Details:\")\n",
        "        for task in self.tasks:\n",
        "            status_emoji = {\n",
        "                TaskStatus.COMPLETED: \"✅\",\n",
        "                TaskStatus.FAILED: \"❌\",\n",
        "                TaskStatus.PENDING: \"⏳\",\n",
        "                TaskStatus.IN_PROGRESS: \"🔄\"\n",
        "            }\n",
        "            print(f\"  {status_emoji[task.status]} {task.id}. {task.description}\")\n",
        "\n",
        "        # Check if plots were created\n",
        "        if os.path.exists('plots') and any(f.endswith('.png') for f in os.listdir('plots')):\n",
        "            print(\"\\\\n🎨 Generated Visualizations:\")\n",
        "            for file in os.listdir('plots'):\n",
        "                if file.endswith('.png'):\n",
        "                    print(f\"  📊 plots/{file}\")\n",
        "\n",
        "\n",
        "# Example usage.\n",
        "if __name__ == \"__main__\":\n",
        "    # Example objectives you can try:\n",
        "    objectives = [\n",
        "        \"Perform data analysis on a sample dataset with visualizations\",\n",
        "        \"Create a simple file processing algorithm\",\n",
        "        \"Build a basic web application structure\",\n",
        "        \"Implement a sorting algorithm with tests\"\n",
        "    ]\n",
        "\n",
        "    print(\"Available example objectives:\")\n",
        "    for i, obj in enumerate(objectives, 1):\n",
        "        print(f\"{i}. {obj}\")\n",
        "\n",
        "    # Use the first objective as default.\n",
        "    selected_objective = objectives[0]\n",
        "    print(f\"\\\\nRunning with objective: {selected_objective}\")\n",
        "\n",
        "    # Create and run the agent.\n",
        "    agent = PythonAutoGPT(selected_objective)\n",
        "    agent.run()\n"
      ],
      "metadata": {
        "id": "2fSXXKi738DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}