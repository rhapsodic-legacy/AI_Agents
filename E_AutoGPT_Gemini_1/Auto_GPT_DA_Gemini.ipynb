{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "# Try different import approaches for Google Generative AI.\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    GENAI_AVAILABLE = True\n",
        "    print(\"✅ Google Generative AI imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"❌ Google Generative AI not available. Install with: pip install google-generativeai\")\n",
        "    GENAI_AVAILABLE = False\n",
        "\n",
        "class TaskStatus(Enum):\n",
        "    PENDING = \"pending\"\n",
        "    IN_PROGRESS = \"in_progress\"\n",
        "    COMPLETED = \"completed\"\n",
        "    FAILED = \"failed\"\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    id: int\n",
        "    description: str\n",
        "    status: TaskStatus\n",
        "    code_to_execute: str = \"\"\n",
        "    result: str = \"\"\n",
        "    error: str = \"\"\n",
        "\n",
        "class GeminiAutoGPT:\n",
        "    def __init__(self, objective: str, api_key: str = None):\n",
        "        self.objective = objective\n",
        "        self.tasks: List[Task] = []\n",
        "        self.task_counter = 0\n",
        "        self.max_iterations = 10\n",
        "        self.current_iteration = 0\n",
        "        self.use_gemini = False\n",
        "\n",
        "        # Configure Gemini if available.\n",
        "        if GENAI_AVAILABLE and api_key and api_key != \"YOUR_GEMINI_API_KEY_HERE\":\n",
        "            try:\n",
        "                genai.configure(api_key=api_key)\n",
        "                self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "                self.use_gemini = True\n",
        "                print(f\"✅ Gemini API configured successfully!\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Warning: Could not configure Gemini API: {e}\")\n",
        "                print(\"Falling back to predefined tasks...\")\n",
        "                self.use_gemini = False\n",
        "        else:\n",
        "            print(\"⚠️ Gemini API not available. Using predefined tasks...\")\n",
        "            self.use_gemini = False\n",
        "\n",
        "    def generate_tasks(self) -> List[str]:\n",
        "        \"\"\"Generate initial tasks based on the objective using Gemini or fallback\"\"\"\n",
        "        if self.use_gemini:\n",
        "            print(\"🤖 Asking Gemini to break down the objective into tasks...\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            You are an AI assistant that breaks down objectives into specific, actionable tasks for a Python AutoGPT agent.\n",
        "\n",
        "            Objective: {self.objective}\n",
        "\n",
        "            Please break this objective down into 4-6 specific, sequential tasks that a Python script can execute.\n",
        "            Each task should be:\n",
        "            1. Specific and actionable\n",
        "            2. Suitable for Python code execution\n",
        "            3. Building upon previous tasks\n",
        "            4. Clear and concise (one sentence each)\n",
        "\n",
        "            Respond with ONLY a JSON array of task descriptions, like this:\n",
        "            [\"Task 1 description\", \"Task 2 description\", \"Task 3 description\", ...]\n",
        "\n",
        "            Do not include any other text or formatting.\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.model.generate_content(prompt)\n",
        "                task_descriptions = json.loads(response.text.strip())\n",
        "                print(f\"✅ Gemini generated {len(task_descriptions)} tasks\")\n",
        "                return task_descriptions\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error generating tasks with Gemini: {e}\")\n",
        "                print(\"Falling back to default tasks...\")\n",
        "\n",
        "        # Fallback tasks based on objective type.\n",
        "        if \"data analysis\" in self.objective.lower() or \"dataset\" in self.objective.lower():\n",
        "            return [\n",
        "                \"Import necessary libraries and set up the environment for data analysis\",\n",
        "                \"Generate a comprehensive sample dataset with realistic business metrics\",\n",
        "                \"Perform exploratory data analysis with descriptive statistics\",\n",
        "                \"Create multiple visualizations including distribution plots and correlation heatmaps\",\n",
        "                \"Generate advanced statistical insights and trend analysis\",\n",
        "                \"Create a summary report with key findings and recommendations\"\n",
        "            ]\n",
        "        elif \"machine learning\" in self.objective.lower():\n",
        "            return [\n",
        "                \"Import ML libraries and prepare the environment\",\n",
        "                \"Generate or load a suitable dataset for machine learning\",\n",
        "                \"Perform data preprocessing and feature engineering\",\n",
        "                \"Train multiple ML models and compare performance\",\n",
        "                \"Evaluate models with cross-validation and metrics\",\n",
        "                \"Create visualizations of model performance and predictions\"\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                \"Import necessary libraries and set up the environment\",\n",
        "                \"Generate sample data relevant to the objective\",\n",
        "                \"Perform basic data exploration and analysis\",\n",
        "                \"Create informative visualizations and charts\",\n",
        "                \"Generate summary statistics and insights\",\n",
        "                \"Create a final report with conclusions\"\n",
        "            ]\n",
        "\n",
        "    def create_task(self, description: str) -> Task:\n",
        "        \"\"\"Create a new task\"\"\"\n",
        "        self.task_counter += 1\n",
        "        task = Task(\n",
        "            id=self.task_counter,\n",
        "            description=description,\n",
        "            status=TaskStatus.PENDING\n",
        "        )\n",
        "        self.tasks.append(task)\n",
        "        return task\n",
        "\n",
        "    def generate_code_for_task(self, task: Task) -> str:\n",
        "        \"\"\"Generate Python code for a given task using Gemini or predefined templates\"\"\"\n",
        "        if self.use_gemini:\n",
        "            print(f\"🤖 Asking Gemini to generate code for: {task.description}\")\n",
        "\n",
        "            # Get context from previous successful tasks.\n",
        "            previous_context = \"\"\n",
        "            for prev_task in self.tasks:\n",
        "                if prev_task.status == TaskStatus.COMPLETED and prev_task.id < task.id:\n",
        "                    previous_context += f\"Task {prev_task.id}: {prev_task.description} - COMPLETED\\n\"\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            You are a Python code generator for an AutoGPT agent. Generate complete, executable Python code for the given task.\n",
        "\n",
        "            Overall Objective: {self.objective}\n",
        "            Current Task: {task.description}\n",
        "\n",
        "            Previous completed tasks:\n",
        "            {previous_context}\n",
        "\n",
        "            Requirements:\n",
        "            1. Generate complete, working Python code that accomplishes the task\n",
        "            2. Include proper error handling and informative print statements\n",
        "            3. Use libraries like pandas, numpy, matplotlib, seaborn as needed\n",
        "            4. For data analysis tasks, create sample data if no data exists\n",
        "            5. For visualization tasks, save plots to a 'plots' directory\n",
        "            6. Use matplotlib with 'Agg' backend for non-interactive plotting\n",
        "            7. Include detailed comments explaining the code\n",
        "            8. Make the code robust and handle edge cases\n",
        "\n",
        "            CRITICAL - For data generation and visualization:\n",
        "            - Create realistic datasets with meaningful column names (not A, B, C)\n",
        "            - Use domain-specific column names related to the objective\n",
        "            - Ensure all plots have proper titles, axis labels, and legends\n",
        "            - Use descriptive titles that explain what the visualization shows\n",
        "            - Always label axes with the actual column names and units where appropriate\n",
        "            - Add proper legends when using multiple series or categories\n",
        "            - Make visualizations publication-ready with good formatting\n",
        "\n",
        "            Respond with ONLY the Python code, no additional text or formatting.\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                response = self.model.generate_content(prompt)\n",
        "                code = response.text.strip()\n",
        "\n",
        "                # Clean up the code if it's wrapped in markdown.\n",
        "                if code.startswith(\"```python\"):\n",
        "                    code = code[9:]\n",
        "                if code.endswith(\"```\"):\n",
        "                    code = code[:-3]\n",
        "\n",
        "                print(f\"✅ Gemini generated {len(code.split())} lines of code\")\n",
        "                return code.strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error generating code with Gemini: {e}\")\n",
        "                print(\"Falling back to template code...\")\n",
        "\n",
        "        # Fallback to predefined code templates.\n",
        "        return self.get_template_code(task)\n",
        "\n",
        "    def get_template_code(self, task: Task) -> str:\n",
        "        \"\"\"Generate template code based on task description\"\"\"\n",
        "        task_desc = task.description.lower()\n",
        "\n",
        "        if \"import\" in task_desc and \"libraries\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Import necessary libraries and set up environment\n",
        "print(\"Setting up environment and importing libraries...\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n",
        "print(\"✅ Directories created!\")\n",
        "\"\"\"\n",
        "\n",
        "        elif \"generate\" in task_desc and \"data\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Generate comprehensive sample dataset\n",
        "print(\"Generating realistic sample dataset...\")\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Generate sample business data\n",
        "n_samples = 1000\n",
        "dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='D')\n",
        "\n",
        "# Create realistic business metrics\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'sales_revenue': np.random.normal(50000, 15000, n_samples).clip(min=0),\n",
        "    'customers': np.random.poisson(200, n_samples),\n",
        "    'marketing_spend': np.random.normal(8000, 2000, n_samples).clip(min=0),\n",
        "    'employee_count': np.random.randint(50, 200, n_samples),\n",
        "    'product_units_sold': np.random.poisson(500, n_samples),\n",
        "    'customer_satisfaction': np.random.normal(4.2, 0.8, n_samples).clip(1, 5),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
        "    'season': pd.Categorical([\n",
        "        'Winter' if month in [12, 1, 2] else\n",
        "        'Spring' if month in [3, 4, 5] else\n",
        "        'Summer' if month in [6, 7, 8] else 'Fall'\n",
        "        for month in dates.month\n",
        "    ])\n",
        "})\n",
        "\n",
        "# Add some realistic correlations\n",
        "df['profit_margin'] = (df['sales_revenue'] - df['marketing_spend']) / df['sales_revenue']\n",
        "df['revenue_per_customer'] = df['sales_revenue'] / df['customers']\n",
        "\n",
        "print(f\"✅ Generated dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Save the dataset\n",
        "df.to_csv('data/business_data.csv', index=False)\n",
        "print(\"✅ Dataset saved to data/business_data.csv\")\n",
        "\"\"\"\n",
        "\n",
        "        elif \"exploratory\" in task_desc or \"descriptive\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Perform exploratory data analysis\n",
        "print(\"Performing exploratory data analysis...\")\n",
        "\n",
        "# Load data if it exists, otherwise use current df\n",
        "try:\n",
        "    if os.path.exists('data/business_data.csv'):\n",
        "        df = pd.read_csv('data/business_data.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        print(\"✅ Loaded dataset from file\")\n",
        "    elif 'df' not in globals():\n",
        "        print(\"❌ No dataset found, cannot perform analysis\")\n",
        "        raise Exception(\"No dataset available\")\n",
        "    else:\n",
        "        print(\"✅ Using existing dataset\")\n",
        "except:\n",
        "    print(\"❌ Error loading dataset\")\n",
        "    raise\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\\\n=== DATASET OVERVIEW ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\\\nColumn types:\\\\n{df.dtypes}\")\n",
        "\n",
        "print(\"\\\\n=== MISSING VALUES ===\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found\")\n",
        "\n",
        "print(\"\\\\n=== DESCRIPTIVE STATISTICS ===\")\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "print(df[numeric_columns].describe())\n",
        "\n",
        "print(\"\\\\n=== CATEGORICAL VARIABLES ===\")\n",
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\\\n{col}:\")\n",
        "    print(df[col].value_counts().head())\n",
        "\n",
        "# Calculate correlations\n",
        "print(\"\\\\n=== CORRELATIONS ===\")\n",
        "correlation_matrix = df[numeric_columns].corr()\n",
        "print(\"Top 5 strongest correlations:\")\n",
        "correlation_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        correlation_pairs.append((\n",
        "            correlation_matrix.columns[i],\n",
        "            correlation_matrix.columns[j],\n",
        "            correlation_matrix.iloc[i, j]\n",
        "        ))\n",
        "\n",
        "correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "for col1, col2, corr in correlation_pairs[:5]:\n",
        "    print(f\"{col1} vs {col2}: {corr:.3f}\")\n",
        "\n",
        "print(\"\\\\n✅ Exploratory data analysis completed!\")\n",
        "\"\"\"\n",
        "\n",
        "        elif \"visualization\" in task_desc or \"chart\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Create comprehensive visualizations\n",
        "print(\"Creating comprehensive visualizations...\")\n",
        "\n",
        "# Load data if it exists\n",
        "try:\n",
        "    if os.path.exists('data/business_data.csv'):\n",
        "        df = pd.read_csv('data/business_data.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        print(\"✅ Loaded dataset from file\")\n",
        "    elif 'df' not in globals():\n",
        "        print(\"❌ No dataset found, cannot create visualizations\")\n",
        "        raise Exception(\"No dataset available\")\n",
        "    else:\n",
        "        print(\"✅ Using existing dataset\")\n",
        "except:\n",
        "    print(\"❌ Error loading dataset\")\n",
        "    raise\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create multiple visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Business Data Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Sales Revenue Over Time\n",
        "axes[0, 0].plot(df['date'], df['sales_revenue'], alpha=0.7, linewidth=1)\n",
        "axes[0, 0].set_title('Sales Revenue Over Time', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Sales Revenue ($)')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Revenue Distribution by Region\n",
        "df.boxplot(column='sales_revenue', by='region', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Sales Revenue Distribution by Region', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Region')\n",
        "axes[0, 1].set_ylabel('Sales Revenue ($)')\n",
        "\n",
        "# 3. Customer Satisfaction vs Revenue\n",
        "scatter = axes[1, 0].scatter(df['customer_satisfaction'], df['sales_revenue'],\n",
        "                           alpha=0.6, c=df['customers'], cmap='viridis')\n",
        "axes[1, 0].set_title('Customer Satisfaction vs Revenue', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Customer Satisfaction Score')\n",
        "axes[1, 0].set_ylabel('Sales Revenue ($)')\n",
        "plt.colorbar(scatter, ax=axes[1, 0], label='Number of Customers')\n",
        "\n",
        "# 4. Seasonal Revenue Pattern\n",
        "seasonal_avg = df.groupby('season')['sales_revenue'].mean()\n",
        "axes[1, 1].bar(seasonal_avg.index, seasonal_avg.values, color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
        "axes[1, 1].set_title('Average Revenue by Season', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Season')\n",
        "axes[1, 1].set_ylabel('Average Sales Revenue ($)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/business_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix of Business Metrics', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"✅ Visualizations created and saved to plots/ directory!\")\n",
        "print(\"Created files:\")\n",
        "print(\"  - plots/business_dashboard.png\")\n",
        "print(\"  - plots/correlation_heatmap.png\")\n",
        "\"\"\"\n",
        "\n",
        "        elif \"statistical\" in task_desc and \"insights\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Generate advanced statistical insights and trend analysis\n",
        "print(\"Generating advanced statistical insights and trend analysis...\")\n",
        "\n",
        "# Load data if it exists\n",
        "try:\n",
        "    if os.path.exists('data/business_data.csv'):\n",
        "        df = pd.read_csv('data/business_data.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        print(\"✅ Loaded dataset from file\")\n",
        "    else:\n",
        "        print(\"❌ No dataset found, cannot perform statistical analysis\")\n",
        "        raise Exception(\"No dataset available\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "print(\"\\\\n=== ADVANCED STATISTICAL INSIGHTS ===\")\n",
        "\n",
        "# 1. Time Series Analysis\n",
        "print(\"\\\\n1. TIME SERIES TREND ANALYSIS\")\n",
        "df_sorted = df.sort_values('date')\n",
        "df_sorted['revenue_ma_7'] = df_sorted['sales_revenue'].rolling(window=7).mean()\n",
        "df_sorted['revenue_ma_30'] = df_sorted['sales_revenue'].rolling(window=30).mean()\n",
        "\n",
        "# Calculate growth rates\n",
        "df_sorted['revenue_growth_rate'] = df_sorted['sales_revenue'].pct_change()\n",
        "avg_growth_rate = df_sorted['revenue_growth_rate'].mean() * 100\n",
        "print(f\"Average daily revenue growth rate: {avg_growth_rate:.2f}%\")\n",
        "\n",
        "# Seasonal analysis\n",
        "monthly_revenue = df_sorted.groupby(df_sorted['date'].dt.month)['sales_revenue'].agg(['mean', 'std'])\n",
        "best_month = monthly_revenue['mean'].idxmax()\n",
        "worst_month = monthly_revenue['mean'].idxmin()\n",
        "print(f\"Best performing month: {best_month} (avg: ${monthly_revenue.loc[best_month, 'mean']:,.2f})\")\n",
        "print(f\"Worst performing month: {worst_month} (avg: ${monthly_revenue.loc[worst_month, 'mean']:,.2f})\")\n",
        "\n",
        "# 2. Statistical Tests\n",
        "print(\"\\\\n2. STATISTICAL HYPOTHESIS TESTS\")\n",
        "\n",
        "# Test for regional differences in revenue\n",
        "regions = df['region'].unique()\n",
        "regional_revenues = [df[df['region'] == region]['sales_revenue'] for region in regions]\n",
        "f_stat, p_value = stats.f_oneway(*regional_revenues)\n",
        "print(f\"ANOVA test for regional revenue differences:\")\n",
        "print(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.6f}\")\n",
        "print(f\"Result: {'Significant' if p_value < 0.05 else 'Not significant'} regional differences\")\n",
        "\n",
        "# Correlation significance tests\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "significant_correlations = []\n",
        "for i, col1 in enumerate(numeric_cols):\n",
        "    for col2 in numeric_cols[i+1:]:\n",
        "        corr, p_val = stats.pearsonr(df[col1].dropna(), df[col2].dropna())\n",
        "        if p_val < 0.05:\n",
        "            significant_correlations.append((col1, col2, corr, p_val))\n",
        "\n",
        "print(f\"\\\\nSignificant correlations (p < 0.05):\")\n",
        "for col1, col2, corr, p_val in sorted(significant_correlations, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
        "    print(f\"{col1} vs {col2}: r={corr:.3f}, p={p_val:.6f}\")\n",
        "\n",
        "# 3. Principal Component Analysis\n",
        "print(\"\\\\n3. PRINCIPAL COMPONENT ANALYSIS\")\n",
        "numeric_data = df[numeric_cols].dropna()\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numeric_data)\n",
        "\n",
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(f\"First 3 components explain {sum(explained_variance[:3]):.1%} of variance\")\n",
        "print(\"Component importance:\")\n",
        "for i, var in enumerate(explained_variance[:3]):\n",
        "    print(f\"  PC{i+1}: {var:.1%}\")\n",
        "\n",
        "# Feature importance in first component\n",
        "feature_importance = abs(pca.components_[0])\n",
        "important_features = sorted(zip(numeric_cols, feature_importance), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\\\nMost important features in PC1:\")\n",
        "for feature, importance in important_features[:5]:\n",
        "    print(f\"  {feature}: {importance:.3f}\")\n",
        "\n",
        "# 4. Outlier Analysis\n",
        "print(\"\\\\n4. OUTLIER ANALYSIS\")\n",
        "from scipy.stats import zscore\n",
        "\n",
        "outlier_counts = {}\n",
        "for col in numeric_cols:\n",
        "    z_scores = np.abs(zscore(df[col].dropna()))\n",
        "    outliers = np.sum(z_scores > 3)\n",
        "    outlier_counts[col] = outliers\n",
        "    if outliers > 0:\n",
        "        outlier_percentage = (outliers / len(df[col].dropna())) * 100\n",
        "        print(f\"{col}: {outliers} outliers ({outlier_percentage:.1f}%)\")\n",
        "\n",
        "# 5. Business Insights\n",
        "print(\"\\\\n5. KEY BUSINESS INSIGHTS\")\n",
        "\n",
        "# Revenue efficiency metrics\n",
        "df['revenue_per_employee'] = df['sales_revenue'] / df['employee_count']\n",
        "df['marketing_roi'] = (df['sales_revenue'] - df['marketing_spend']) / df['marketing_spend']\n",
        "\n",
        "avg_revenue_per_employee = df['revenue_per_employee'].mean()\n",
        "avg_marketing_roi = df['marketing_roi'].mean()\n",
        "\n",
        "print(f\"Average revenue per employee: ${avg_revenue_per_employee:,.2f}\")\n",
        "print(f\"Average marketing ROI: {avg_marketing_roi:.2f}x\")\n",
        "\n",
        "# Segment analysis\n",
        "high_satisfaction = df[df['customer_satisfaction'] > 4.5]['sales_revenue'].mean()\n",
        "low_satisfaction = df[df['customer_satisfaction'] <= 3.5]['sales_revenue'].mean()\n",
        "satisfaction_impact = ((high_satisfaction - low_satisfaction) / low_satisfaction) * 100\n",
        "\n",
        "print(f\"High satisfaction customers generate {satisfaction_impact:.1f}% more revenue\")\n",
        "\n",
        "# Seasonal patterns\n",
        "seasonal_stats = df.groupby('season').agg({\n",
        "    'sales_revenue': ['mean', 'std'],\n",
        "    'customer_satisfaction': 'mean',\n",
        "    'marketing_spend': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\\\nSeasonal performance summary:\")\n",
        "for season in seasonal_stats.index:\n",
        "    revenue_mean = seasonal_stats.loc[season, ('sales_revenue', 'mean')]\n",
        "    satisfaction_mean = seasonal_stats.loc[season, ('customer_satisfaction', 'mean')]\n",
        "    print(f\"{season}: Avg Revenue ${revenue_mean:,.2f}, Satisfaction {satisfaction_mean:.2f}\")\n",
        "\n",
        "print(\"\\\\n✅ Advanced statistical analysis completed!\")\n",
        "\"\"\"\n",
        "\n",
        "        elif \"summary\" in task_desc and \"report\" in task_desc:\n",
        "            return \"\"\"\n",
        "# Task: Create a summary report with key findings and recommendations\n",
        "print(\"Creating comprehensive summary report...\")\n",
        "\n",
        "# Load data if it exists\n",
        "try:\n",
        "    if os.path.exists('data/business_data.csv'):\n",
        "        df = pd.read_csv('data/business_data.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        print(\"✅ Loaded dataset from file\")\n",
        "    else:\n",
        "        print(\"❌ No dataset found, cannot create report\")\n",
        "        raise Exception(\"No dataset available\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Generate summary report\n",
        "report = []\n",
        "report.append(\"=\" * 80)\n",
        "report.append(\"BUSINESS DATA ANALYSIS - EXECUTIVE SUMMARY REPORT\")\n",
        "report.append(\"=\" * 80)\n",
        "report.append(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "report.append(f\"Analysis Period: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "report.append(f\"Total Records Analyzed: {len(df):,}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Key Performance Metrics\n",
        "report.append(\"KEY PERFORMANCE METRICS\")\n",
        "report.append(\"-\" * 40)\n",
        "report.append(f\"Total Revenue: ${df['sales_revenue'].sum():,.2f}\")\n",
        "report.append(f\"Average Daily Revenue: ${df['sales_revenue'].mean():,.2f}\")\n",
        "report.append(f\"Revenue Standard Deviation: ${df['sales_revenue'].std():,.2f}\")\n",
        "report.append(f\"Total Customers Served: {df['customers'].sum():,}\")\n",
        "report.append(f\"Average Customer Satisfaction: {df['customer_satisfaction'].mean():.2f}/5.0\")\n",
        "report.append(f\"Total Marketing Investment: ${df['marketing_spend'].sum():,.2f}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Regional Performance\n",
        "report.append(\"REGIONAL PERFORMANCE\")\n",
        "report.append(\"-\" * 40)\n",
        "regional_summary = df.groupby('region').agg({\n",
        "    'sales_revenue': ['mean', 'sum'],\n",
        "    'customers': 'sum',\n",
        "    'customer_satisfaction': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "for region in regional_summary.index:\n",
        "    avg_revenue = regional_summary.loc[region, ('sales_revenue', 'mean')]\n",
        "    total_revenue = regional_summary.loc[region, ('sales_revenue', 'sum')]\n",
        "    total_customers = regional_summary.loc[region, ('customers', 'sum')]\n",
        "    avg_satisfaction = regional_summary.loc[region, ('customer_satisfaction', 'mean')]\n",
        "\n",
        "    report.append(f\"{region} Region:\")\n",
        "    report.append(f\"  • Average Daily Revenue: ${avg_revenue:,.2f}\")\n",
        "    report.append(f\"  • Total Revenue: ${total_revenue:,.2f}\")\n",
        "    report.append(f\"  • Total Customers: {total_customers:,}\")\n",
        "    report.append(f\"  • Avg Satisfaction: {avg_satisfaction:.2f}/5.0\")\n",
        "\n",
        "report.append(\"\")\n",
        "\n",
        "# Seasonal Analysis\n",
        "report.append(\"SEASONAL TRENDS\")\n",
        "report.append(\"-\" * 40)\n",
        "seasonal_summary = df.groupby('season').agg({\n",
        "    'sales_revenue': 'mean',\n",
        "    'customer_satisfaction': 'mean',\n",
        "    'marketing_spend': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "best_season = seasonal_summary['sales_revenue'].idxmax()\n",
        "worst_season = seasonal_summary['sales_revenue'].idxmin()\n",
        "\n",
        "report.append(f\"Best Performing Season: {best_season}\")\n",
        "report.append(f\"  • Average Revenue: ${seasonal_summary.loc[best_season, 'sales_revenue']:,.2f}\")\n",
        "report.append(f\"Lowest Performing Season: {worst_season}\")\n",
        "report.append(f\"  • Average Revenue: ${seasonal_summary.loc[worst_season, 'sales_revenue']:,.2f}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Key Correlations\n",
        "report.append(\"KEY BUSINESS CORRELATIONS\")\n",
        "report.append(\"-\" * 40)\n",
        "correlations = df.corr()['sales_revenue'].abs().sort_values(ascending=False)[1:6]  # Exclude self-correlation\n",
        "for var, corr in correlations.items():\n",
        "    strength = \"Strong\" if corr > 0.7 else \"Moderate\" if corr > 0.4 else \"Weak\"\n",
        "    report.append(f\"• {var}: {strength} correlation ({corr:.3f})\")\n",
        "\n",
        "report.append(\"\")\n",
        "\n",
        "# Business Insights and Recommendations\n",
        "report.append(\"KEY INSIGHTS & RECOMMENDATIONS\")\n",
        "report.append(\"-\" * 40)\n",
        "\n",
        "# Calculate some insights\n",
        "high_satisfaction_revenue = df[df['customer_satisfaction'] > 4.0]['sales_revenue'].mean()\n",
        "low_satisfaction_revenue = df[df['customer_satisfaction'] <= 3.0]['sales_revenue'].mean()\n",
        "satisfaction_premium = ((high_satisfaction_revenue - low_satisfaction_revenue) / low_satisfaction_revenue) * 100\n",
        "\n",
        "df['marketing_roi'] = (df['sales_revenue'] - df['marketing_spend']) / df['marketing_spend']\n",
        "avg_roi = df['marketing_roi'].mean()\n",
        "\n",
        "insights = [\n",
        "    f\"1. Customer satisfaction drives revenue - high satisfaction customers generate {satisfaction_premium:.1f}% more revenue\",\n",
        "    f\"2. Current marketing ROI is {avg_roi:.2f}x, indicating {'strong' if avg_roi > 2 else 'moderate' if avg_roi > 1 else 'poor'} marketing efficiency\",\n",
        "    f\"3. {best_season} season shows strongest performance - consider increasing marketing during this period\",\n",
        "    f\"4. Revenue variability (${df['sales_revenue'].std():,.0f}) suggests opportunity for process optimization\",\n",
        "    f\"5. {regional_summary['sales_revenue']['mean'].idxmax()} region outperforms others - analyze best practices for replication\"\n",
        "]\n",
        "\n",
        "for insight in insights:\n",
        "    report.append(insight)\n",
        "\n",
        "report.append(\"\")\n",
        "\n",
        "# Recommendations\n",
        "report.append(\"STRATEGIC RECOMMENDATIONS\")\n",
        "report.append(\"-\" * 40)\n",
        "recommendations = [\n",
        "    \"• Focus on customer satisfaction initiatives to drive revenue growth\",\n",
        "    \"• Optimize marketing spend allocation based on seasonal performance patterns\",\n",
        "    \"• Investigate regional success factors for knowledge transfer\",\n",
        "    \"• Implement process improvements to reduce revenue variability\",\n",
        "    \"• Develop predictive models for better demand forecasting\"\n",
        "]\n",
        "\n",
        "for rec in recommendations:\n",
        "    report.append(rec)\n",
        "\n",
        "# Save and display report\n",
        "report_content = \"\\\\n\".join(report)\n",
        "with open('business_analysis_report.txt', 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(\"\\\\n\" + report_content)\n",
        "print(f\"\\\\n✅ Summary report generated and saved to 'business_analysis_report.txt'\")\n",
        "\"\"\"\n",
        "\n",
        "        else:\n",
        "            # Generic template - Fixed the df reference issue.\n",
        "            return f\"\"\"\n",
        "# Task: {task.description}\n",
        "print(\"Executing task: {task.description}\")\n",
        "\n",
        "try:\n",
        "    # Basic task implementation\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    print(\"Libraries imported successfully\")\n",
        "\n",
        "    # Check if data file exists and load it\n",
        "    if os.path.exists('data/business_data.csv'):\n",
        "        df = pd.read_csv('data/business_data.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        print(f\"Working with dataset: {{df.shape}}\")\n",
        "    else:\n",
        "        print(\"No dataset file found\")\n",
        "\n",
        "    print(\"Task completed successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in task execution: {e}\")\n",
        "\"\"\"\n",
        "\n",
        "    def execute_code(self, code: str) -> tuple[str, str]:\n",
        "        \"\"\"Execute Python code and return output and error\"\"\"\n",
        "        try:\n",
        "            # Build cumulative code by combining all previous successful task codes.\n",
        "            cumulative_code = \"\"\n",
        "\n",
        "            # Add all previously executed successful task codes.\n",
        "            for task in self.tasks:\n",
        "                if task.status == TaskStatus.COMPLETED and task.code_to_execute:\n",
        "                    cumulative_code += task.code_to_execute + \"\\n\\n\"\n",
        "\n",
        "            # Add current code.\n",
        "            full_code = cumulative_code + code\n",
        "\n",
        "            # Create a temporary file to execute the code.\n",
        "            with open('temp_code.py', 'w') as f:\n",
        "                f.write(full_code)\n",
        "\n",
        "            # Execute the code and capture output.\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, 'temp_code.py'],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            # Clean up\n",
        "            if os.path.exists('temp_code.py'):\n",
        "                os.remove('temp_code.py')\n",
        "\n",
        "            return result.stdout, result.stderr\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return \"\", \"Code execution timed out\"\n",
        "        except Exception as e:\n",
        "            return \"\", f\"Execution error: {str(e)}\"\n",
        "\n",
        "    def execute_task(self, task: Task) -> bool:\n",
        "        \"\"\"Execute a single task\"\"\"\n",
        "        print(f\"\\n--- Executing Task {task.id}: {task.description} ---\")\n",
        "\n",
        "        task.status = TaskStatus.IN_PROGRESS\n",
        "\n",
        "        # Generate code for the task.\n",
        "        task.code_to_execute = self.generate_code_for_task(task)\n",
        "\n",
        "        print(\"Generated code preview (first 10 lines):\")\n",
        "        print(\"```python\")\n",
        "        code_lines = task.code_to_execute.split('\\n')\n",
        "        for i, line in enumerate(code_lines[:10]):\n",
        "            print(line)\n",
        "        if len(code_lines) > 10:\n",
        "            print(f\"... ({len(code_lines) - 10} more lines)\")\n",
        "        print(\"```\")\n",
        "\n",
        "        # Execute the code.\n",
        "        output, error = self.execute_code(task.code_to_execute)\n",
        "\n",
        "        task.result = output\n",
        "        task.error = error\n",
        "\n",
        "        if error and \"warning\" not in error.lower():\n",
        "            print(f\"❌ Task failed with error: {error}\")\n",
        "            task.status = TaskStatus.FAILED\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"✅ Task completed successfully!\")\n",
        "            if output:\n",
        "                print(\"Output preview (last 20 lines):\")\n",
        "                output_lines = output.split('\\n')\n",
        "                for line in output_lines[-20:]:\n",
        "                    if line.strip():\n",
        "                        print(line)\n",
        "            task.status = TaskStatus.COMPLETED\n",
        "            return True\n",
        "\n",
        "    def should_continue(self) -> bool:\n",
        "        \"\"\"Determine if the agent should continue with more tasks\"\"\"\n",
        "        if self.current_iteration >= self.max_iterations:\n",
        "            print(f\"\\n🛑 Reached maximum iterations ({self.max_iterations})\")\n",
        "            return False\n",
        "\n",
        "        pending_tasks = [t for t in self.tasks if t.status == TaskStatus.PENDING]\n",
        "        if not pending_tasks:\n",
        "            print(\"\\n✅ All tasks completed!\")\n",
        "            return False\n",
        "\n",
        "        failed_tasks = [t for t in self.tasks if t.status == TaskStatus.FAILED]\n",
        "        if len(failed_tasks) > len(self.tasks) * 0.5:\n",
        "            print(\"\\n❌ Too many tasks failed, stopping execution\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main execution loop\"\"\"\n",
        "        print(f\"🚀 Starting {'Gemini-Powered' if self.use_gemini else 'Template-Based'} AutoGPT Agent\")\n",
        "        print(f\"Objective: {self.objective}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Generate initial tasks.\n",
        "        task_descriptions = self.generate_tasks()\n",
        "        for desc in task_descriptions:\n",
        "            self.create_task(desc)\n",
        "\n",
        "        print(f\"\\n📋 Generated {len(self.tasks)} tasks:\")\n",
        "        for task in self.tasks:\n",
        "            print(f\"  {task.id}. {task.description}\")\n",
        "\n",
        "        # Execute tasks.\n",
        "        while self.should_continue():\n",
        "            self.current_iteration += 1\n",
        "            print(f\"\\n🔄 Iteration {self.current_iteration}\")\n",
        "\n",
        "            # Find next pending task.\n",
        "            pending_tasks = [t for t in self.tasks if t.status == TaskStatus.PENDING]\n",
        "            if not pending_tasks:\n",
        "                break\n",
        "\n",
        "            current_task = pending_tasks[0]\n",
        "            success = self.execute_task(current_task)\n",
        "\n",
        "            # Brief pause between tasks.\n",
        "            time.sleep(1)\n",
        "\n",
        "        # Final summary.\n",
        "        self.print_summary()\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print execution summary\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"📊 EXECUTION SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        completed = len([t for t in self.tasks if t.status == TaskStatus.COMPLETED])\n",
        "        failed = len([t for t in self.tasks if t.status == TaskStatus.FAILED])\n",
        "        pending = len([t for t in self.tasks if t.status == TaskStatus.PENDING])\n",
        "\n",
        "        print(f\"Objective: {self.objective}\")\n",
        "        print(f\"Total Tasks: {len(self.tasks)}\")\n",
        "        print(f\"✅ Completed: {completed}\")\n",
        "        print(f\"❌ Failed: {failed}\")\n",
        "        print(f\"⏳ Pending: {pending}\")\n",
        "        print(f\"🔄 Iterations: {self.current_iteration}\")\n",
        "\n",
        "        print(\"\\nTask Details:\")\n",
        "        for task in self.tasks:\n",
        "            status_emoji = {\n",
        "                TaskStatus.COMPLETED: \"✅\",\n",
        "                TaskStatus.FAILED: \"❌\",\n",
        "                TaskStatus.PENDING: \"⏳\",\n",
        "                TaskStatus.IN_PROGRESS: \"🔄\"\n",
        "            }\n",
        "            print(f\"  {status_emoji[task.status]} {task.id}. {task.description}\")\n",
        "\n",
        "        # Check if plots were created.\n",
        "        if os.path.exists('plots') and any(f.endswith('.png') for f in os.listdir('plots')):\n",
        "            print(\"\\n🎨 Generated Visualizations:\")\n",
        "            for file in os.listdir('plots'):\n",
        "                if file.endswith('.png'):\n",
        "                    file_size = os.path.getsize(os.path.join('plots', file)) / 1024\n",
        "                    print(f\"  📊 plots/{file} ({file_size:.1f} KB)\")\n",
        "\n",
        "        # Check for any other generated files.\n",
        "        generated_files = []\n",
        "        for root, dirs, files in os.walk('.'):\n",
        "            for file in files:\n",
        "                if file.endswith(('.csv', '.json', '.txt', '.html')) and not file.startswith('temp_'):\n",
        "                    generated_files.append(os.path.join(root, file))\n",
        "\n",
        "        if generated_files:\n",
        "            print(\"\\n📄 Other Generated Files:\")\n",
        "            for file in generated_files:\n",
        "                file_size = os.path.getsize(file) / 1024\n",
        "                print(f\"  📄 {file} ({file_size:.1f} KB)\")\n",
        "\n",
        "\n",
        "# Example usage.\n",
        "if __name__ == \"__main__\":\n",
        "    # Example objectives.\n",
        "    objectives = [\n",
        "        \"Perform comprehensive data analysis on a sample dataset with multiple visualizations and statistical insights\",\n",
        "        \"Create a machine learning pipeline for classification or regression with model evaluation\",\n",
        "        \"Build a data processing system that can handle multiple file formats and generate reports\",\n",
        "        \"Develop a time series analysis with forecasting and trend visualization\",\n",
        "        \"Create an automated data quality assessment tool with detailed reporting\"\n",
        "    ]\n",
        "\n",
        "    print(\"🤖 AutoGPT Agent (Gemini-Enhanced)\")\n",
        "    print(\"Available example objectives:\")\n",
        "    for i, obj in enumerate(objectives, 1):\n",
        "        print(f\"{i}. {obj}\")\n",
        "\n",
        "    # Use the first objective as default.\n",
        "    selected_objective = objectives[0]\n",
        "    print(f\"\\nRunning with objective: {selected_objective}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Optional: Set your Gemini API key here.\n",
        "    # GEMINI_API_KEY = \"YOUR_KEY_HERE\"  # Uncomment and add your key.\n",
        "    GEMINI_API_KEY = None  # Will use template-based approach.\n",
        "\n",
        "    # Create and run the agent\n",
        "    agent = GeminiAutoGPT(selected_objective, GEMINI_API_KEY)\n",
        "    agent.run()"
      ],
      "metadata": {
        "id": "iD6bspZ8IgLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}